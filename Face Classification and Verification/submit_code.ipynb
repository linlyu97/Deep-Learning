{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision   \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Block\n",
    "\n",
    "Resnet: https://arxiv.org/pdf/1512.03385.pdf\n",
    "\n",
    "Here is a basic usage of shortcut in Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channel_size, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_size, channel_size, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channel_size)\n",
    "        self.shortcut = nn.Conv2d(channel_size, channel_size, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn1(self.conv1(out))\n",
    "        x = self.shortcut(x)\n",
    "#         print(\"out: \", str(out.size()))\n",
    "#         print(\"x: \", str(x.size()))\n",
    "        out += x\n",
    "        out = F.relu(out)\n",
    "        #print(out.size())\n",
    "        #print('***********')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model with Residual Block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, num_feats, hidden_sizes, num_classes, feat_dim=10):\n",
    "        super(Network, self).__init__()\n",
    "        self.hidden_sizes = [num_feats] + hidden_sizes + [num_classes]\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Conv2d(in_channels=3, out_channels= 32, kernel_size=3, stride=1, bias=False))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "        self.layers.append(BasicBlock(channel_size = 32))\n",
    "                           \n",
    "        self.layers.append(nn.Conv2d(in_channels=32, out_channels= 64, kernel_size=3, stride=2, bias=False))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "        self.layers.append(BasicBlock(channel_size = 64))\n",
    "        \n",
    "        self.layers.append(nn.Conv2d(in_channels=64, out_channels= 128, kernel_size=3, stride=2, bias=False))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "        self.layers.append(BasicBlock(channel_size = 128))\n",
    "        \n",
    "        self.layers.append(nn.Conv2d(in_channels=128, out_channels= 256, kernel_size=3, stride=1, bias=False))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "        self.layers.append(BasicBlock(channel_size = 256))\n",
    "        \n",
    "        self.layers.append(nn.Conv2d(in_channels=256, out_channels= 512, kernel_size=3, stride=1, bias=False))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "        self.layers.append(BasicBlock(channel_size = 512))\n",
    "                           \n",
    "        self.layers.append(nn.Conv2d(in_channels=512, out_channels= 1024, kernel_size=3, stride=1, bias=False))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "        self.layers.append(BasicBlock(channel_size = 1024))\n",
    "                           \n",
    "        self.layers.append(nn.Conv2d(in_channels=1024, out_channels= 512, kernel_size=3, stride=1, bias=False))\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "        self.layers.append(BasicBlock(channel_size = 512))\n",
    "        \n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.emb_layer = nn.Linear(self.hidden_sizes[-2], 512, bias=False)\n",
    "        self.linear_label = nn.Linear(512, self.hidden_sizes[-1], bias=True)\n",
    "        self.dp = nn.Dropout(0.3)\n",
    "        \n",
    "        # For creating the embedding to be passed into the Center Loss criterion\n",
    "        self.linear_closs = nn.Linear(self.hidden_sizes[-2], feat_dim, bias=False)\n",
    "        self.relu_closs = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x, evalMode=False):\n",
    "        output = x # (B, 3, W, H)\n",
    "#         print(output.size())\n",
    "        output = self.layers(output)  # (B, 512, k, k)\\\n",
    "        #print(output.size())\n",
    "        output = F.avg_pool2d(output, [output.size(2), output.size(3)], stride=1) # b 512 1 \n",
    "        output = output.reshape(output.shape[0], output.shape[1]) #b 512\n",
    "        output_emb = self.emb_layer(output) # 512 512\n",
    "        output = self.dp(output_emb)\n",
    "        label_output = self.linear_label(output) # 512 4000\n",
    "        label_output = label_output/torch.norm(self.linear_label.weight, dim=1)\n",
    "        \n",
    "        # Create the feature embedding for the Center Loss\n",
    "        closs_output = self.linear_closs(output)\n",
    "        closs_output = self.relu_closs(closs_output)\n",
    "\n",
    "        return closs_output, label_output, output_emb\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#start = 16\n",
    "def train(model, data_loader, test_loader, task='Classification'):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in tqdm(enumerate(data_loader)):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "#             print(\"feats: \", str(feats.shape))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(feats)[1] # _call function, implement forward\n",
    "            \n",
    "\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if batch_num % 1000 == 999:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/1000))\n",
    "                avg_loss = 0.0    \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        \n",
    "        if task == 'Classification':\n",
    "            val_loss, val_acc = test_classify(model, test_loader)\n",
    "            train_loss, train_acc = test_classify(model, data_loader)\n",
    "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
    "                  format(train_loss, train_acc, val_loss, val_acc))\n",
    "        else:\n",
    "            test_verify(model, test_loader)\n",
    "        end_time = time.time()\n",
    "        print('Time: ',end_time - start_time, 's')\n",
    "        print('='*20)\n",
    "    \n",
    "        # may save the training model for future use\n",
    "        if not os.path.exists(\"./model\"):\n",
    "            os.mkdir(\"./model\")\n",
    "\n",
    "        torch.save(model.state_dict(),'/home/ubuntu/11785/hw2/model/{}_{}_{}'.format('MODEL_NAME',epoch, 'flip'))\n",
    "        logging.info('model saved to ./model/{}_{}_{}'.format('MODEL_NAME',epoch,'flip'))\n",
    "        \n",
    "        res = get_cosine_simil(model, val_veri_dataloader)\n",
    "        print('AUC score', roc_auc_score(np.asarray(label_list), res))\n",
    "\n",
    "\n",
    "def test_classify(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "        feats, labels = feats.to(device), labels.to(device)\n",
    "        outputs = model(feats)[1]\n",
    "        \n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "        test_loss.extend([loss.item()]*feats.size()[0])\n",
    "        del feats\n",
    "        del labels\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total\n",
    "\n",
    "\n",
    "def test_verify(model, test_loader):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset, DataLoader and Constant Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform_test = torchvision.transforms.Compose([torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                                      torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "                                                      torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root='/home/ubuntu/11785/hw2/classification_data/train_data/', \n",
    "                                                 transform=data_transform_test )\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, \n",
    "                                               shuffle=True, num_workers=0)\n",
    "\n",
    "dev_dataset = torchvision.datasets.ImageFolder(root='/home/ubuntu/11785/hw2/classification_data/val_data/', \n",
    "                                               transform=torchvision.transforms.ToTensor())\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=64, \n",
    "                                             shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 25\n",
    "num_feats = 3\n",
    "\n",
    "learningRate = 1e-3\n",
    "weightDecay = 5e-5\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network(num_feats, hidden_sizes, num_classes)\n",
    "network.to(device)\n",
    "network.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first train, after epoch 15, val accuracy stucks\n",
    "network.train()\n",
    "train(network, train_dataloader, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the first train, epoch 15 model, change lr to 1e-3\n",
    "network.load_state_dict(torch.load(\"./model/MODEL_NAME_15\"))\n",
    "# second train based on first train epoch 15 model, trained 3 epoch\n",
    "network.train()\n",
    "train(network, train_dataloader, dev_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom DataSet with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, file_list1):\n",
    "        self.file_list = file_list\n",
    "        self.file_list1 = file_list1\n",
    "        #self.n_class = len(list(set(target_list)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.file_list[index])\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        img1 = Image.open(self.file_list1[index])\n",
    "        img1 = torchvision.transforms.ToTensor()(img1)\n",
    "        #label = self.target_list[index]\n",
    "        return img, img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifi_txt = np.loadtxt('verification_pairs_val.txt', delimiter = ' ', dtype=bytes).astype(str)\n",
    "image_list = verifi_txt[:,0]\n",
    "image_list1 = verifi_txt[:,1]\n",
    "label_list = verifi_txt[:,2]\n",
    "val_veri_dataset = ImageDataset(image_list, image_list1)\n",
    "val_veri_dataloader = DataLoader(val_veri_dataset, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifi_txt_test = np.loadtxt('verification_pairs_test.txt', delimiter = ' ', dtype=bytes).astype(str)\n",
    "image_list = verifi_txt_test[:,0]\n",
    "image_list1 = verifi_txt_test[:,1]\n",
    "\n",
    "test_veri_dataset = ImageDataset(image_list, image_list1)\n",
    "test_veri_dataloader = DataLoader(test_veri_dataset, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_simil(model, dataloader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for batch_num, (img, img1) in tqdm(enumerate(dataloader)):\n",
    "        img, img1 = img.to(device), img1.to(device)\n",
    "        img_emb_feature = model(img)[2].detach() # batch_size * 512(# of neurons)\n",
    "        img1_emb_feature = model(img1)[2].detach()  # batch_size * 512\n",
    "        cos = nn.CosineSimilarity(dim=1)\n",
    "        output = cos(img_emb_feature, img1_emb_feature) # batch_size \n",
    "        del img\n",
    "        del img1\n",
    "        del img_emb_feature\n",
    "        del img1_emb_feature\n",
    "        result.append(output.cpu())\n",
    "        #result.append(output)\n",
    "    \n",
    "    results = torch.cat(result)\n",
    "    results = results.numpy() # number of verification pairs\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data augmentaion based on random horizontal flip and vertical flip\n",
    "# third train, based on second train best model, trained 7 epoch\n",
    "network.load_state_dict(torch.load(\"./model/MODEL_NAME_17\"))\n",
    "network.train()\n",
    "train(network, train_dataloader, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fccd14a07d41b0bd24d039bbd7564e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "network.load_state_dict(torch.load(\"./model/MODEL_NAME_5_flip\"))\n",
    "test_res = get_cosine_simil(network, test_veri_dataloader)\n",
    "import pandas as pd\n",
    "sub = pd.read_csv('verification_pairs_test.txt', header = None)\n",
    "submit = sub.set_axis(['Id'], axis=1, inplace=False)\n",
    "submit['Category'] = test_res\n",
    "submit.to_csv(\"submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Center Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, feat_dim, device=torch.device('cpu')):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).to(self.device))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long().to(self.device)\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = []\n",
    "        for i in range(batch_size):\n",
    "            value = distmat[i][mask[i]]\n",
    "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability\n",
    "            dist.append(value)\n",
    "        dist = torch.cat(dist)\n",
    "        loss = dist.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_closs(model, data_loader, test_loader, task='Classification'):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in tqdm(enumerate(data_loader)):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer_label.zero_grad()\n",
    "            optimizer_closs.zero_grad()\n",
    "            \n",
    "            feature, outputs, _ = model(feats)\n",
    "\n",
    "            l_loss = criterion_label(outputs, labels.long())\n",
    "            c_loss = criterion_closs(feature, labels.long())\n",
    "            loss = l_loss + closs_weight * c_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_label.step()\n",
    "            # by doing so, weight_cent would not impact on the learning of centers\n",
    "            for param in criterion_closs.parameters():\n",
    "                param.grad.data *= (1. / closs_weight)\n",
    "            optimizer_closs.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if batch_num % 1000 == 999:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/1000))\n",
    "                avg_loss = 0.0    \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        \n",
    "        if not os.path.exists(\"./model\"):\n",
    "            os.mkdir(\"./model\")\n",
    "\n",
    "        torch.save(model.state_dict(),'/home/ubuntu/11785/hw2/model/{}_{}_{}'.format('MODEL_NAME',epoch, 'center'))\n",
    "        logging.info('model saved to ./model/{}_{}_{}'.format('MODEL_NAME',epoch,'center'))\n",
    "        \n",
    "        res = get_cosine_simil(model, val_veri_dataloader)\n",
    "        print('AUC score', roc_auc_score(np.asarray(label_list), res))\n",
    "        \n",
    "        if task == 'Classification':\n",
    "            val_loss, val_acc = test_classify_closs(model, test_loader)\n",
    "            train_loss, train_acc = test_classify_closs(model, data_loader)\n",
    "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
    "                  format(train_loss, train_acc, val_loss, val_acc))\n",
    "        else:\n",
    "            test_verify(model, test_loader)\n",
    "\n",
    "\n",
    "def test_classify_closs(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "        feats, labels = feats.to(device), labels.to(device)\n",
    "        feature, outputs, _ = model(feats)\n",
    "        \n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        l_loss = criterion_label(outputs, labels.long())\n",
    "        c_loss = criterion_closs(feature, labels.long())\n",
    "        loss = l_loss + closs_weight * c_loss\n",
    "        \n",
    "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "        test_loss.extend([loss.item()]*feats.size()[0])\n",
    "        del feats\n",
    "        del labels\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closs_weight = 1\n",
    "lr_cent = 0.01\n",
    "feat_dim = 10\n",
    "\n",
    "network = Network(num_feats, hidden_sizes, num_classes, feat_dim)\n",
    "network.apply(init_weights)\n",
    "\n",
    "criterion_label = nn.CrossEntropyLoss()\n",
    "criterion_closs = CenterLoss(num_classes, feat_dim, device)\n",
    "optimizer_label = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
    "optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=lr_cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_state_dict(torch.load(\"./model/MODEL_NAME_5_flip\"))\n",
    "network.train()\n",
    "network.to(device)\n",
    "train_closs(network, train_dataloader, dev_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
